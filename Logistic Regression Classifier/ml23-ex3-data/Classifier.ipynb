{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Starter code for exercise 5: Logistic Model for Argument Quality\n",
    "##################################################################\n",
    "\n",
    "GROUP = \"04\"  # TODO: write in your group number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_vectors(filename: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Load the feature vectors from the dataset in the given file and return\n",
    "    them as a numpy array with shape (number-of-examples, number-of-features + 1).\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    # load_feature_vectors reads feature vectors from a features-*-cleaned.tsv and returns the contained multiset of feature vectors X as an n-by-(p+1) matrix\n",
    "    data = pd.read_csv(filename, delimiter='\\t')\n",
    "    features = np.hstack([np.ones((data.shape[0], 1)), data.values])\n",
    "    return features\n",
    "\n",
    "# # print features\n",
    "# print(load_feature_vectors(\"features-train-cleaned.tsv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_values(filename: str) -> np.array:\n",
    "    \"\"\"\n",
    "    Load the class values for overall quality (class 0 for quality 1 and class 1\n",
    "    for overall quality 2 or 3) from the dataset in the given file and return\n",
    "    them as a one-dimensional numpy array.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    # Load data\n",
    "    data = pd.read_csv(filename, delimiter='\\t')\n",
    "\n",
    "    # Check if 'overall quality' column exists\n",
    "    if 'overall quality' not in data.columns:\n",
    "        raise KeyError(\"Column 'overall quality' not found in the dataset.\")\n",
    "\n",
    "    # Create class values based on the 'overall quality' column\n",
    "    classes = np.where(data['overall quality'] > 1, 1, 0)\n",
    "    return classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_rate(cs: np.array, ys: np.array) -> float:\n",
    "    \"\"\"\n",
    "    This function takes two vectors with gold and predicted labels and\n",
    "    returns the percentage of positions where truth and prediction disagree\n",
    "    \"\"\"\n",
    "    if len(cs) == 0:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        # TODO: Your code here\n",
    "        return np.sum(cs) / len(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(w: np.array, x: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Return the output of a logistic function with parameter vector `w` on\n",
    "    example `x`.\n",
    "    Hint: use np.exp(np.clip(..., -30, 30)) instead of np.exp(...) to avoid\n",
    "    divisions by zero\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    z = np.clip(np.dot(x, w.T), -30, 30)\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prediction(w: np.array, x: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Making predictions based on the output of the logistic function\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    return np.round(logistic_function(w, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random_weights(p: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Generate a pseudorandom weight vector of dimension p.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    return np.random.uniform(-1, 1, (p + 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(w: np.array, x: np.array, c: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the logistic loss function\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    return -np.sum(c * np.log(logistic_function(w, x)) + (1 - c) * np.log(1 - logistic_function(w, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_with_bgd(xs: np.array, cs: np.array, eta: float=1e-8, iterations: int=1000, validation_fraction: float=0) -> Tuple[np.array, float, float]:\n",
    "    \"\"\"\n",
    "    Fit a logistic regression model using the Batch Gradient Descent algorithm and\n",
    "    return the learned weights as a numpy array.\n",
    "\n",
    "    Arguments:\n",
    "    - `xs`: feature vectors in the training dataset as a two-dimensional numpy array with shape (n, p+1)\n",
    "    - `cs`: class values c(x) for every element in `xs` as a one-dimensional numpy array with length n\n",
    "    - `eta`: the learning rate as a float value\n",
    "    - `iterations': the number of iterations to run the algorithm for\n",
    "    - 'validation_fraction': fraction of xs and cs used for validation (not for training)\n",
    "\n",
    "    Returns:\n",
    "    - the learned weights as a column vector, i.e. a two-dimensional numpy array with shape (1, p)\n",
    "    - logistic loss value\n",
    "    - misclassification rate of predictions on training part of xs/cs\n",
    "    - misclassification rate of predictions on validation part of xs/cs\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "     # Initialize weights\n",
    "    w = initialize_random_weights(xs.shape[1])\n",
    "\n",
    "    # Your implementation for BGD training\n",
    "\n",
    "    loss = logistic_loss(w, xs, cs)\n",
    "    train_misclassification_rate = misclassification_rate(logistic_prediction(w, xs), cs)\n",
    "    \n",
    "    # Define validation_xs and validation_cs here\n",
    "    validation_xs = ...\n",
    "    validation_cs = ...\n",
    "    \n",
    "    validation_misclassification_rate = misclassification_rate(logistic_prediction(w, validation_xs), validation_cs)\n",
    "\n",
    "    return w, loss, train_misclassification_rate, validation_misclassification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_misclassification_rates(losss: List[float], train_misclassification_rates: List[float], validation_misclassification_rates: List[float]):\n",
    "    \"\"\"\n",
    "    Plots the normalized loss (divided by max(losss)) and both misclassification rates\n",
    "    for each iteration.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losss / np.max(losss), label='Normalized Loss')\n",
    "    plt.title('Normalized Loss Over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Normalized Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_misclassification_rates, label='Training Misclassification Rate')\n",
    "    plt.plot(validation_misclassification_rates, label='Validation Misclassification Rate')\n",
    "    plt.title('Misclassification Rates Over Iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Misclassification Rate')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a)\n",
      "(b)\n",
      "(c)\n",
      "\u001b[1m============================= test session starts =============================\u001b[0m\n",
      "platform win32 -- Python 3.11.3, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: e:\\Hobby_Projects\\Machine Learning\\Introduction-to-Machine-Learning-\\Logistic Regression Classifier\\ml23-ex3-data\n",
      "plugins: anyio-3.6.2, typeguard-2.13.3\n",
      "collected 0 items\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ============================\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: found no collectors for e:\\Hobby_Projects\\Machine Learning\\Introduction-to-Machine-Learning-\\Logistic Regression Classifier\\ml23-ex3-data\\Classifier.ipynb\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darsh\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Tests\n",
    "import os\n",
    "from pytest import approx\n",
    "\n",
    "\n",
    "def test_logistic_function():\n",
    "    x = np.array([1, 1, 2])\n",
    "    assert logistic_function(np.array([0, 0, 0]), x) == approx(0.5)\n",
    "    assert logistic_function(np.array([1e2, 1e2, 1e2]), x) == approx(1)\n",
    "    assert logistic_function(np.array([-1e2, -1e2, -1e2]), x) == approx(0)\n",
    "    assert logistic_function(np.array([1e2, -1e2, 0]), x) == approx(0.5)\n",
    "\n",
    "\n",
    "def test_bgd():\n",
    "    xs = np.array([\n",
    "        [1, -1],\n",
    "        [1, 2],\n",
    "        [1, -2],\n",
    "    ])\n",
    "    cs = np.array([0, 1, 0])\n",
    "    \n",
    "    w, _, _, _ = train_logistic_regression_with_bgd(xs, cs, 0.1, 100)\n",
    "    assert w @ [1, -1] < 0 and w @ [1, 2] > 0\n",
    "    w, _, _, _ = train_logistic_regression_with_bgd(-xs, cs, 0.1, 100)\n",
    "    assert w @ [1, -1] > 0 and w @ [1, 2] < 0\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Main program for running against the training dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    import pytest\n",
    "    import sys\n",
    "\n",
    "    # Set your file names here (adjust as needed)\n",
    "    train_features_file_name = \"features-train-cleaned.tsv\"\n",
    "    train_classes_file_name = \"quality-scores-train-cleaned.tsv\"\n",
    "    test_features_file_name = \"features-test-cleaned.tsv\"\n",
    "    test_predictions_file_name = \"quality-scores-test-predicted.tsv\"\n",
    "\n",
    "    # Load data\n",
    "    xs = load_feature_vectors(train_features_file_name)\n",
    "    xs_test = load_feature_vectors(test_features_file_name)\n",
    "    cs = load_class_values(train_classes_file_name)\n",
    "\n",
    "    print(\"(a)\")\n",
    "    # TODO: Print number of examples with each class\n",
    "\n",
    "    print(\"(b)\")\n",
    "    # TODO: Print misclassification rate of random classifier\n",
    "\n",
    "    print(\"(c)\")\n",
    "    test_c_result = pytest.main(['-k', 'test_logistic_function', '--tb=short', 'Classifier.ipynb'])\n",
    "    if test_c_result != 0:\n",
    "        sys.exit(test_c_result)\n",
    "    print(\"Test logistic function successful\")\n",
    "\n",
    "    print(\"(d)\")\n",
    "    test_d_result = pytest.main(['-k', 'test_bgd', '--tb=short', 'Classifier.ipynb'])\n",
    "    if test_d_result != 0:\n",
    "        sys.exit(test_d_result)\n",
    "    print(\"Test bgd successful\")\n",
    "\n",
    "    print(\"(e)\")\n",
    "    w, losss, train_misclassification_rates, validation_misclassification_rates = train_logistic_regression_with_bgd(xs, cs, validation_fraction=0.2)\n",
    "    plot_loss_and_misclassification_rates(losss, train_misclassification_rates, validation_misclassification_rates)\n",
    "\n",
    "    print(\"(f)\")\n",
    "    # (f) Predict on test set and write to test_predictions_file_name\n",
    "    w, _, _, _ = train_logistic_regression_with_bgd(xs, cs, validation_fraction=0.2)\n",
    "    predictions = np.round(logistic_function(w, xs_test))\n",
    "\n",
    "    # Write predictions to file\n",
    "    pd.DataFrame(predictions).to_csv(test_predictions_file_name, header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
